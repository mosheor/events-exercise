# evidences-exercise

Home assignment on dynamic parsing of raw evidences data

The task is described in [here](_Backend%20Exercise.docx).

* Summary of set up

1. brew install pipx pre-commit
2. pipx install poetry
3. cd `project`
4. poetry env use python3.12 (or path to python 3.12)
5. poetry install
6. pre-commit install
7. pre-commit install-hooks

### Build Docker image ###

```shell
docker build  .
```

#### OR docker compose ####

```shell
docker-compose up -d --build
```

##### Configuration #####

* All projects configuration is stored in [pyproject.toml](pyproject.toml)
* Dependencies:

```shell
* poetry add {package} ...
```

### Run migrations ###

```shell
poetry run beanie new-migration -n migration_name -p evidence_handler/migrations
poetry run beanie migrate -uri 'mongodb://root:password@localhost' -p evidence_handler/migrations -db evidence_handler
poetry run beanie migrate -uri 'mongodb://root:password@localhost' -p evidence_handler/migrations -db evidence_handler --backward
```

### Adding new configuration ###

Example of generating a configuration json
in: [evidence_handler/scripts/prepare_config.py](evidence_handler/scripts/prepare_config.py)
Steps for adding/modifying configuration:

1. Create a new instance of the Serializer class within evidence_id_to_serializer.
2. Specify the evidence ID as the key in evidence_id_to_serializer.
3. Configure Fields for the Serializer instance: Inside the new Serializer instance, define a list of fields with their
   respective configurations. You can use different field types (e.g., StringField, IntegerField, etc.) based on your
   requirements.
4. Specify Field Configurations: For each field, create an instance of the field class (e.g., StringField, IntegerField)
   with specific parameters like field_name and output_field.
5. Add these field instances to the fields list of the serializer.
6. Update the JSON Output File: After modifying the evidence_id_to_serializer dictionary, run the script again to
   generate a new JSON configuration file.

# Exersice assumptions and explanations

- I decided to use MongoDB as a database, because it is a document-oriented database, which is a good fit for the task.
- I used Beanie as an ODM. it is a modern and fast ODM for MongoDB, which is based on Pydantic.
- I used FastAPI as a web framework, because it is a modern, asynchronous and fast web framework.
- I decided to save the data in the database as it is, without any changes, because it is way more flexible if we want
  to change the data visualization format in the future - there will be no need to change the data in the database,
  and we can just change the serialization configuration.
- The schema is applied on read, not on write - maybe it is not the best solution performance-wise, but it is the most
  flexible one.
- Assumption: the given examples of serializer fields are representing the configuration possibilities, if other
  serialization formats are required, they will need to be supported with new field classes.
- The configuration is stored in a JSON file, which is generated by a script. I decided to use this format and not to
  store it in the database, because it is more flexible and easy to configure and should come from outside the
  application.
